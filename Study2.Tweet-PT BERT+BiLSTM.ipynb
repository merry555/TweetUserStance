{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "test = pd.read_csv('./datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['label']!=2]\n",
    "test = test[test['label']!=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@eyfydsyd97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-longformer-the-long-document-transformer-e9ade1980536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, stratify=train.label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet = train[['tweet','username','label']]\n",
    "test_tweet = test[['tweet','username','label']]\n",
    "valid_tweet = valid[['tweet','username','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet = train[['tweet','username','label']].groupby(['username','label'])['tweet'].apply(list).reset_index(name='text')\n",
    "valid_tweet = valid[['tweet','username','label']].groupby(['username','label'])['tweet'].apply(list).reset_index(name='text')\n",
    "test_tweet = test[['tweet','username','label']].groupby(['username','label'])['tweet'].apply(list).reset_index(name='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet = train_tweet.reset_index()\n",
    "test_tweet = test_tweet.reset_index()\n",
    "valid_tweet = valid_tweet.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_tweet\n",
    "test = test_tweet\n",
    "valid = valid_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "# Internal dependencies\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "####BERT\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache() \n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    #device = torch.device(\"cuda\") # select the zeroth GPU with this line: gpu = 0\n",
    "    #device = torch.cuda.set_device(1)  #wrong provide device = None  \n",
    "    device = torch.device(1) #(use cuda device 1) for gpu = 1\n",
    "    torch.cuda.set_device(device)\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  cuda:1\n"
     ]
    }
   ],
   "source": [
    "print ('Current cuda device ', device) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.cuda.current_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  9 02:17:27 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\r\n",
      "| 51%   84C    P2   186W / 250W |   1666MiB / 11019MiB |     72%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\r\n",
      "| 49%   69C    P8    19W / 250W |     11MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 47%   67C    P8    20W / 250W |   3054MiB / 11016MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     24304      C   ...dxlab/jupyter/envs/grad_py36/bin/python  1655MiB |\r\n",
      "|    2     23406      C   ...dxlab/jupyter/envs/grad_py36/bin/python  3043MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### text preprocessing #########\n",
    "\n",
    "def remove_string_noise(input_str):\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    punctuation_noise =\"!\\\"$%&'#()*+,-./:;<=>?@[\\]^_`{|}~\" #print string.punctuation \n",
    "    special_noise = \"\"\n",
    "\n",
    "    all_noise = punctuation_noise + special_noise\n",
    "\n",
    "    for c in all_noise:\n",
    "        if c in input_str:\n",
    "            input_str = input_str.replace(c, \" \")#replace with space\n",
    "    fresh_str = ' '.join(input_str.split())\n",
    "    return fresh_str\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sw = set(stopwords.words('english'))\n",
    "    operators = set(('no', 'not', 'nor', 'none'))\n",
    "    stop_words = set(sw) - operators\n",
    "    stop_words.update([ 'amp', 'rt'])  ###as we are using set so we used .update....otherwise .extends\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #if w not in stop_words and w not in emoticons:\n",
    "        if w not in stop_words:\n",
    "            filtered_tweet.append(w)\n",
    "    lemmatized_tweet = []\n",
    "    for word in filtered_tweet:\n",
    "        lemmatized_tweet.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    return ' '.join(lemmatized_tweet)\n",
    "    \n",
    "def pre_processing_tweets (df):    \n",
    "    clean_text = []\n",
    "\n",
    "    for i in df:\n",
    "        clean_text.append(p.clean(str(i)))#python 3\n",
    "    fresh_text1 = []\n",
    "    for i in range (0, df.shape[0]):\n",
    "        fresh_text1.append(remove_string_noise(clean_text[i]))\n",
    "    \n",
    "    filtered_tweet = []\n",
    "    for i in range (0, len(fresh_text1)):\n",
    "        filtered_tweet.append(clean_tweets(fresh_text1[i].lower()))\n",
    "    \n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = pre_processing_tweets(train.text)\n",
    "sentences_valid = pre_processing_tweets(valid.text)\n",
    "sentences_test = pre_processing_tweets(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sentences_train = np.array(sentences_train)\n",
    "labels_train = train.label.values\n",
    "print(type(sentences_train))\n",
    "\n",
    "sentences_valid = np.array(sentences_valid)\n",
    "labels_valid = valid.label.values\n",
    "print(type(sentences_valid))\n",
    "\n",
    "sentences_test = np.array(sentences_test)\n",
    "labels_test = test.label.values\n",
    "print(type(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length train:  2692\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_train:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length train: ', max_len) # 2692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length valid:  622\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_valid:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length valid: ', max_len) # 622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length test:  8973\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_test:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length test: ', max_len) # 8973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_train:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_valid = []\n",
    "attention_masks_valid = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_valid:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_valid.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_valid.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_valid = torch.cat(input_ids_valid, dim=0)\n",
    "attention_masks_valid = torch.cat(attention_masks_valid, dim=0)\n",
    "labels_valid = torch.tensor(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_test:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 8 #good\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size'] #768\n",
    "              \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        #self.fc1 = nn.Linear(150, output_dim) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim) \n",
    "        #self.fc2 = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        #print(\"text\", text.size()) #torch.Size([32, 50])       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        #print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\n",
    "        #embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\n",
    "        lstm_out, (hidden, c_n) = self.rnn(embedded) #for lstm\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #print(\"hidden before\", type(hidden), hidden.size()) #torch.Size([4, 32, 256])\n",
    "        if self.rnn.bidirectional: #add dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "#         if self.rnn.bidirectional: #No dropout\n",
    "#             hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         else:\n",
    "#             hidden = hidden[-1,:,:]\n",
    "            \n",
    "        #lstm_out = self.dropout(lstm_out) #add dropout\n",
    "    \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #print(\"hidden after\", type(hidden), hidden.size()) #torch.Size([32, 512]) cz of bi-direction = 2 * 256 = 512\n",
    "        #attn_out = self.attention(lstm_out, hidden)\n",
    "#         query = hidden = [batch size, output length, dimensions]\n",
    "#         context = lstm_out = [batch size, query length, dimensions]\n",
    "#        input for attention attention(query, context)\n",
    "#       output for attention tuple with output and weights\n",
    "        #attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "        #print(type(attn_out[0]), attn_out[0].size(), type(attn_out[1]), attn_out[1].size()) #output = torch.Size([32, 1, 512]), weight = torch.Size([32, 1, 160])\n",
    "        #print(\"attn_out\", attn_out.size())\n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = self.out(attn_out[0].squeeze(1))\n",
    "        \n",
    "        #output = self.fc1(output)\n",
    "        #output = self.fc2(output)\n",
    "        #output = [batch size, out dim]\n",
    "        #print(\"output\", type(output), output.size()) #torch.Size([32, 3])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HIDDEN_DIM = 256 #original\n",
    "HIDDEN_DIM = 300 #good\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2 #original good 3\n",
    "#N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "#DROPOUT = 0.25 #original\n",
    "DROPOUT = 0.25 #good\n",
    "\n",
    "\n",
    "model = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "    #print(name)\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#learning_rate=0.01\n",
    "# optimizer = optim.Adam(model.parameters()) #original #good #f1 = 64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# optimizer = AdamW(model.parameters(),\n",
    "#                   lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        label = batch[2].to(device)\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        predictions = model(text)\n",
    "        #pred = model(text)\n",
    "        #print(\"pred\", pred.size()) #torch.Size([32, 3])\n",
    "        #predictions = torch.argmax(pred, dim = 1)\n",
    "        #print(\"predictions\", predictions, predictions.size()) #torch.Size([32, 3])\n",
    "        #loss = criterion(predictions, batch.label)\n",
    "        loss = criterion(predictions, label)\n",
    "        #print(\"loss\", loss)\n",
    "        #acc = binary_accuracy(predictions, batch.label)\n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "        #print(\"acc\", acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        preds , true_labels = [], []\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            text = batch[0].to(device)\n",
    "            label = batch[2].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(text)\n",
    "            \n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds.append(torch.argmax(predictions, dim = 1).to('cpu').numpy())\n",
    "            true_labels.append(label.to('cpu').numpy())\n",
    "            \n",
    "                        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved in epoch : 1\n",
      "Epoch: 01 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.697 | Train Acc: 57.27 | Train macro-avg-f1: 50.68%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 49.13 |  Val. macro-avg-f1: 34.24%\n",
      "Epoch: 02 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.667 | Train Acc: 60.36 | Train macro-avg-f1: 55.59%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 48.93 |  Val. macro-avg-f1: 33.02%\n",
      "Epoch: 03 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.652 | Train Acc: 62.63 | Train macro-avg-f1: 57.26%\n",
      "\t Val. Loss: 0.835 |  Val. Acc: 48.93 |  Val. macro-avg-f1: 33.02%\n",
      "Epoch: 04 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.617 | Train Acc: 66.29 | Train macro-avg-f1: 63.06%\n",
      "\t Val. Loss: 0.984 |  Val. Acc: 49.44 |  Val. macro-avg-f1: 34.18%\n",
      "Epoch: 05 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.590 | Train Acc: 69.39 | Train macro-avg-f1: 64.12%\n",
      "\t Val. Loss: 0.774 |  Val. Acc: 52.44 |  Val. macro-avg-f1: 41.27%\n",
      "best model saved in epoch : 6\n",
      "Epoch: 06 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.578 | Train Acc: 70.74 | Train macro-avg-f1: 66.71%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 58.83 |  Val. macro-avg-f1: 51.86%\n",
      "best model saved in epoch : 7\n",
      "Epoch: 07 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.536 | Train Acc: 73.90 | Train macro-avg-f1: 70.25%\n",
      "\t Val. Loss: 0.605 |  Val. Acc: 68.29 |  Val. macro-avg-f1: 65.21%\n",
      "Epoch: 08 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.524 | Train Acc: 75.28 | Train macro-avg-f1: 72.22%\n",
      "\t Val. Loss: 0.650 |  Val. Acc: 60.81 |  Val. macro-avg-f1: 54.86%\n",
      "Epoch: 09 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.493 | Train Acc: 77.54 | Train macro-avg-f1: 74.65%\n",
      "\t Val. Loss: 0.717 |  Val. Acc: 55.77 |  Val. macro-avg-f1: 46.26%\n",
      "Epoch: 10 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.462 | Train Acc: 79.36 | Train macro-avg-f1: 76.36%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 56.01 |  Val. macro-avg-f1: 46.00%\n",
      "best model saved in epoch : 11\n",
      "Epoch: 11 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.438 | Train Acc: 80.37 | Train macro-avg-f1: 77.46%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 67.37 |  Val. macro-avg-f1: 64.22%\n",
      "Epoch: 12 | Epoch Time: 1m 8s\n",
      "\tTrain Loss: 0.423 | Train Acc: 81.02 | Train macro-avg-f1: 78.14%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 60.24 |  Val. macro-avg-f1: 53.30%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7935df4aa356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-8345a4b8f4b9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#predictions = model(batch.text).squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#loss = criterion(predictions, batch.label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/envs/grad_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-49b340f5bb05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#for lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#hidden = [n layers * n directions, batch size, hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/envs/grad_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/envs/grad_py36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Start training\n",
    "per_epoch_train_loss = []\n",
    "per_epoch_val_loss = []\n",
    "per_epoch_train_f1 = []\n",
    "per_epoch_val_f1 = []\n",
    "per_epoch_train_acc = []\n",
    "per_epoch_val_acc = []\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    per_epoch_train_loss.append(train_loss)\n",
    "    per_epoch_val_loss.append(valid_loss)\n",
    "    per_epoch_train_f1.append(train_f1)\n",
    "    per_epoch_val_f1.append(valid_f1)\n",
    "    per_epoch_train_acc.append(train_acc)\n",
    "    per_epoch_val_acc.append(valid_acc)\n",
    "    \n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"best model saved in epoch :\", epoch+1 )\n",
    "    torch.save(model.state_dict(), 'data/bert_twt/model_twt_wo_attn_'+str(epoch+1)+'.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_losses, val_losses, train_f1, val_f1, train_accs, val_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(1,n+1,1)\n",
    "\n",
    "    # plot train and val losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('data/bert_plot/loss_Bert_Loc_utype_wo_attn.png')\n",
    "\n",
    "    # plot train and val f1-score\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_f1, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_f1, '-', linewidth=2, label='validation')\n",
    "    \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/f1_Bert_Loc_utype_wo_attn.png')\n",
    "    \n",
    "    # plot train and val accuracy\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_accs, '-', linewidth=2, label='validation')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/acc_Bert_Loc_utype_wo_attn.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1, per_epoch_train_acc, per_epoch_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        preds , true_labels = [], []\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            text = batch[0].to(device)\n",
    "            label = batch[2].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(text)\n",
    "            \n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds.append(torch.argmax(predictions, dim = 1).to('cpu').numpy())\n",
    "            true_labels.append(label.to('cpu').numpy())\n",
    "            \n",
    "                        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator), preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for epoch in range(0,15):\n",
    "    model.load_state_dict(torch.load('data/bert_twt/model_twt_wo_attn_'+str(epoch+1)+'.pt'))\n",
    "\n",
    "    test_loss, test_acc, test_f1, pred, label = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.3f} | Test macro-avg-f1: {test_f1*100:.3f}%')\n",
    "    flat_predictions = np.concatenate(pred, axis=0)\n",
    "    flat_labels = np.concatenate(label, axis=0)\n",
    "    \n",
    "    print(classification_report(flat_labels, flat_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(flat_labels, flat_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test Loss: 0.660 | Test Acc: 68.250 | Test macro-avg-f1: 61.566%\n",
    "Test Loss: 0.637 | Test Acc: 72.500 | Test macro-avg-f1: 67.102%\n",
    "Test Loss: 0.656 | Test Acc: 69.250 | Test macro-avg-f1: 62.161%\n",
    "Test Loss: 0.581 | Test Acc: 73.500 | Test macro-avg-f1: 68.485%\n",
    "Test Loss: 0.616 | Test Acc: 69.750 | Test macro-avg-f1: 62.418%\n",
    "Test Loss: 0.580 | Test Acc: 73.750 | Test macro-avg-f1: 69.892%\n",
    "Test Loss: 0.639 | Test Acc: 70.500 | Test macro-avg-f1: 63.049%\n",
    "Test Loss: 0.574 | Test Acc: 74.000 | Test macro-avg-f1: 69.004%\n",
    "Test Loss: 0.587 | Test Acc: 71.750 | Test macro-avg-f1: 64.628%\n",
    "Test Loss: 0.601 | Test Acc: 73.500 | Test macro-avg-f1: 66.236%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_py36",
   "language": "python",
   "name": "grad_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
