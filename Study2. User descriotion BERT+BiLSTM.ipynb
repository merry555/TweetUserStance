{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./datasets/train_user.csv')\n",
    "test = pd.read_csv('./datasets/test_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['label']!=2]\n",
    "test = test[test['label']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, stratify=train.label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "# Internal dependencies\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "####BERT\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache() \n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    #device = torch.device(\"cuda\") # select the zeroth GPU with this line: gpu = 0\n",
    "    #device = torch.cuda.set_device(1)  #wrong provide device = None  \n",
    "    device = torch.device(0) #(use cuda device 1) for gpu = 1\n",
    "    torch.cuda.set_device(device)\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print ('Current cuda device ', device) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.cuda.current_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 17 16:25:10 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\r\n",
      "| 40%   56C    P8    22W / 250W |     11MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\r\n",
      "|ERR!   87C    P2   191W / 250W |   9174MiB / 11019MiB |     99%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 34%   55C    P8    19W / 250W |     11MiB / 11016MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1      6320      C   ...dxlab/jupyter/envs/grad_py36/bin/python  9163MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### text preprocessing #########\n",
    "\n",
    "def remove_string_noise(input_str):\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    punctuation_noise =\"!\\\"$%&'#()*+,-./:;<=>?@[\\]^_`{|}~\" #print string.punctuation \n",
    "    special_noise = \"\"\n",
    "\n",
    "    all_noise = punctuation_noise + special_noise\n",
    "\n",
    "    for c in all_noise:\n",
    "        if c in input_str:\n",
    "            input_str = input_str.replace(c, \" \")#replace with space\n",
    "    fresh_str = ' '.join(input_str.split())\n",
    "    return fresh_str\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sw = set(stopwords.words('english'))\n",
    "    operators = set(('no', 'not', 'nor', 'none'))\n",
    "    stop_words = set(sw) - operators\n",
    "    stop_words.update([ 'amp', 'rt'])  ###as we are using set so we used .update....otherwise .extends\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #if w not in stop_words and w not in emoticons:\n",
    "        if w not in stop_words:\n",
    "            filtered_tweet.append(w)\n",
    "    lemmatized_tweet = []\n",
    "    for word in filtered_tweet:\n",
    "        lemmatized_tweet.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    return ' '.join(lemmatized_tweet)\n",
    "    \n",
    "def pre_processing_tweets (df):    \n",
    "    clean_text = []\n",
    "\n",
    "    for i in df:\n",
    "        clean_text.append(p.clean(str(i)))#python 3\n",
    "    fresh_text1 = []\n",
    "    for i in range (0, df.shape[0]):\n",
    "        fresh_text1.append(remove_string_noise(clean_text[i]))\n",
    "    \n",
    "    filtered_tweet = []\n",
    "    for i in range (0, len(fresh_text1)):\n",
    "        filtered_tweet.append(clean_tweets(fresh_text1[i].lower()))\n",
    "    \n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = pre_processing_tweets(train.description)\n",
    "sentences_valid = pre_processing_tweets(valid.description)\n",
    "sentences_test = pre_processing_tweets(test.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sentences_train = np.array(sentences_train)\n",
    "labels_train = train.label.values\n",
    "print(type(sentences_train))\n",
    "\n",
    "sentences_valid = np.array(sentences_valid)\n",
    "labels_valid = valid.label.values\n",
    "print(type(sentences_valid))\n",
    "\n",
    "sentences_test = np.array(sentences_test)\n",
    "labels_test = test.label.values\n",
    "print(type(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length train:  61\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_train:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length train: ', max_len) # 2692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length valid:  39\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_valid:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length valid: ', max_len) # 622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length test:  43\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_test:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length test: ', max_len) # 8973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_train:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_valid = []\n",
    "attention_masks_valid = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_valid:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_valid.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_valid.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_valid = torch.cat(input_ids_valid, dim=0)\n",
    "attention_masks_valid = torch.cat(attention_masks_valid, dim=0)\n",
    "labels_valid = torch.tensor(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_test:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16 #good 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size'] #768\n",
    "              \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        #self.fc1 = nn.Linear(150, output_dim) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim) \n",
    "        #self.fc2 = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        #print(\"text\", text.size()) #torch.Size([32, 50])       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        #print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\n",
    "        #embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\n",
    "        lstm_out, (hidden, c_n) = self.rnn(embedded) #for lstm\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #print(\"hidden before\", type(hidden), hidden.size()) #torch.Size([4, 32, 256])\n",
    "        if self.rnn.bidirectional: #add dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "#         if self.rnn.bidirectional: #No dropout\n",
    "#             hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         else:\n",
    "#             hidden = hidden[-1,:,:]\n",
    "            \n",
    "        #lstm_out = self.dropout(lstm_out) #add dropout\n",
    "    \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #print(\"hidden after\", type(hidden), hidden.size()) #torch.Size([32, 512]) cz of bi-direction = 2 * 256 = 512\n",
    "        #attn_out = self.attention(lstm_out, hidden)\n",
    "#         query = hidden = [batch size, output length, dimensions]\n",
    "#         context = lstm_out = [batch size, query length, dimensions]\n",
    "#        input for attention attention(query, context)\n",
    "#       output for attention tuple with output and weights\n",
    "        #attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "        #print(type(attn_out[0]), attn_out[0].size(), type(attn_out[1]), attn_out[1].size()) #output = torch.Size([32, 1, 512]), weight = torch.Size([32, 1, 160])\n",
    "        #print(\"attn_out\", attn_out.size())\n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = self.out(attn_out[0].squeeze(1))\n",
    "        \n",
    "        #output = self.fc1(output)\n",
    "        #output = self.fc2(output)\n",
    "        #output = [batch size, out dim]\n",
    "        #print(\"output\", type(output), output.size()) #torch.Size([32, 3])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HIDDEN_DIM = 256 #original\n",
    "HIDDEN_DIM = 256 #good 300\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2 #original good 3\n",
    "#N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "#DROPOUT = 0.25 #original\n",
    "DROPOUT = 0.25 #good\n",
    "\n",
    "\n",
    "model = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "    #print(name)\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# learning_rate=0.0001\n",
    "# optimizer = optim.Adam(model.parameters()) #original #good #f1 = 64%\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=0.0001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 0.002, # args.learning_rate - default is 5e-5, our notebook had 2e-5 원래: 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# 75%: Adam\n",
    "# 75%: lr 0.001, eps = None\n",
    "# 0.001: 55.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-5) #1e-5: best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        text = batch[0].to(device)\n",
    "        label = batch[2].to(device)\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        predictions = model(text)\n",
    "        #pred = model(text)\n",
    "        #print(\"pred\", pred.size()) #torch.Size([32, 3])\n",
    "        #predictions = torch.argmax(pred, dim = 1)\n",
    "        #print(\"predictions\", predictions, predictions.size()) #torch.Size([32, 3])\n",
    "        #loss = criterion(predictions, batch.label)\n",
    "        loss = criterion(predictions, label)\n",
    "        #print(\"loss\", loss)\n",
    "        #acc = binary_accuracy(predictions, batch.label)\n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "        #print(\"acc\", acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        preds , true_labels = [], []\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            text = batch[0].to(device)\n",
    "            label = batch[2].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(text)\n",
    "            \n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds.append(torch.argmax(predictions, dim = 1).to('cpu').numpy())\n",
    "            true_labels.append(label.to('cpu').numpy())\n",
    "            \n",
    "                        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved in epoch : 1\n",
      "Epoch: 01 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.724 | Train Acc: 50.88 | Train macro-avg-f1: 37.19%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 51.65 |  Val. macro-avg-f1: 33.67%\n",
      "best model saved in epoch : 2\n",
      "Epoch: 02 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.701 | Train Acc: 50.23 | Train macro-avg-f1: 42.40%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.66 |  Val. macro-avg-f1: 32.72%\n",
      "best model saved in epoch : 3\n",
      "Epoch: 03 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.697 | Train Acc: 51.22 | Train macro-avg-f1: 41.43%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.59 |  Val. macro-avg-f1: 35.45%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.702 | Train Acc: 50.86 | Train macro-avg-f1: 40.56%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 48.04 |  Val. macro-avg-f1: 32.03%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.696 | Train Acc: 51.45 | Train macro-avg-f1: 44.45%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 48.66 |  Val. macro-avg-f1: 46.66%\n",
      "best model saved in epoch : 6\n",
      "Epoch: 06 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.689 | Train Acc: 54.77 | Train macro-avg-f1: 48.07%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 54.77 |  Val. macro-avg-f1: 53.14%\n",
      "Epoch: 07 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.670 | Train Acc: 59.31 | Train macro-avg-f1: 53.81%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 54.60 |  Val. macro-avg-f1: 53.18%\n",
      "Epoch: 08 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.656 | Train Acc: 60.57 | Train macro-avg-f1: 55.90%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 53.21 |  Val. macro-avg-f1: 38.80%\n",
      "Epoch: 09 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.652 | Train Acc: 60.93 | Train macro-avg-f1: 55.64%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 54.29 |  Val. macro-avg-f1: 50.19%\n",
      "Epoch: 10 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.621 | Train Acc: 64.38 | Train macro-avg-f1: 61.16%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 56.16 |  Val. macro-avg-f1: 47.36%\n",
      "Epoch: 11 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.619 | Train Acc: 64.38 | Train macro-avg-f1: 60.53%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 53.38 |  Val. macro-avg-f1: 45.45%\n",
      "Epoch: 12 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.587 | Train Acc: 65.92 | Train macro-avg-f1: 62.64%\n",
      "\t Val. Loss: 0.759 |  Val. Acc: 53.98 |  Val. macro-avg-f1: 50.64%\n",
      "Epoch: 13 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.565 | Train Acc: 68.09 | Train macro-avg-f1: 65.78%\n",
      "\t Val. Loss: 0.785 |  Val. Acc: 53.21 |  Val. macro-avg-f1: 45.31%\n",
      "Epoch: 14 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.535 | Train Acc: 69.96 | Train macro-avg-f1: 67.34%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 52.27 |  Val. macro-avg-f1: 49.92%\n",
      "Epoch: 15 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.506 | Train Acc: 73.35 | Train macro-avg-f1: 71.97%\n",
      "\t Val. Loss: 0.804 |  Val. Acc: 53.21 |  Val. macro-avg-f1: 51.69%\n"
     ]
    }
   ],
   "source": [
    "###Start training\n",
    "per_epoch_train_loss = []\n",
    "per_epoch_val_loss = []\n",
    "per_epoch_train_f1 = []\n",
    "per_epoch_val_f1 = []\n",
    "per_epoch_train_acc = []\n",
    "per_epoch_val_acc = []\n",
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    per_epoch_train_loss.append(train_loss)\n",
    "    per_epoch_val_loss.append(valid_loss)\n",
    "    per_epoch_train_f1.append(train_f1)\n",
    "    per_epoch_val_f1.append(valid_f1)\n",
    "    per_epoch_train_acc.append(train_acc)\n",
    "    per_epoch_val_acc.append(valid_acc)\n",
    "    \n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"best model saved in epoch :\", epoch+1 )\n",
    "    torch.save(model.state_dict(), 'data/bert_twt/model_des_wo_attn_'+str(epoch+1)+'.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        preds , true_labels = [], []\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            text = batch[0].to(device)\n",
    "            label = batch[2].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(text)\n",
    "            \n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds.append(torch.argmax(predictions, dim = 1).to('cpu').numpy())\n",
    "            true_labels.append(label.to('cpu').numpy())\n",
    "            \n",
    "                        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator), preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.704 | Test Acc: 50.000 | Test macro-avg-f1: 32.580%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5025    1.0000    0.6689       199\n",
      "           1     0.0000    0.0000    0.0000       197\n",
      "\n",
      "    accuracy                         0.5025       396\n",
      "   macro avg     0.2513    0.5000    0.3345       396\n",
      "weighted avg     0.2525    0.5025    0.3361       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.694 | Test Acc: 50.000 | Test macro-avg-f1: 32.633%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       199\n",
      "           1     0.4975    1.0000    0.6644       197\n",
      "\n",
      "    accuracy                         0.4975       396\n",
      "   macro avg     0.2487    0.5000    0.3322       396\n",
      "weighted avg     0.2475    0.4975    0.3305       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.693 | Test Acc: 51.000 | Test macro-avg-f1: 35.836%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5078    0.9849    0.6701       199\n",
      "           1     0.7000    0.0355    0.0676       197\n",
      "\n",
      "    accuracy                         0.5126       396\n",
      "   macro avg     0.6039    0.5102    0.3689       396\n",
      "weighted avg     0.6034    0.5126    0.3704       396\n",
      "\n",
      "Test Loss: 0.695 | Test Acc: 50.250 | Test macro-avg-f1: 33.583%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.0101    0.0198       199\n",
      "           1     0.4987    0.9949    0.6644       197\n",
      "\n",
      "    accuracy                         0.5000       396\n",
      "   macro avg     0.5827    0.5025    0.3421       396\n",
      "weighted avg     0.5831    0.5000    0.3405       396\n",
      "\n",
      "Test Loss: 0.697 | Test Acc: 49.333 | Test macro-avg-f1: 46.700%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4933    0.3719    0.4241       199\n",
      "           1     0.4919    0.6142    0.5463       197\n",
      "\n",
      "    accuracy                         0.4924       396\n",
      "   macro avg     0.4926    0.4930    0.4852       396\n",
      "weighted avg     0.4926    0.4924    0.4849       396\n",
      "\n",
      "Test Loss: 0.682 | Test Acc: 55.333 | Test macro-avg-f1: 52.879%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5421    0.7437    0.6271       199\n",
      "           1     0.5854    0.3655    0.4500       197\n",
      "\n",
      "    accuracy                         0.5556       396\n",
      "   macro avg     0.5637    0.5546    0.5386       396\n",
      "weighted avg     0.5636    0.5556    0.5390       396\n",
      "\n",
      "Test Loss: 0.689 | Test Acc: 54.750 | Test macro-avg-f1: 52.499%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5405    0.7035    0.6114       199\n",
      "           1     0.5693    0.3959    0.4671       197\n",
      "\n",
      "    accuracy                         0.5505       396\n",
      "   macro avg     0.5549    0.5497    0.5392       396\n",
      "weighted avg     0.5549    0.5505    0.5396       396\n",
      "\n",
      "Test Loss: 0.709 | Test Acc: 52.500 | Test macro-avg-f1: 40.711%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5162    0.9598    0.6714       199\n",
      "           1     0.6923    0.0914    0.1614       197\n",
      "\n",
      "    accuracy                         0.5278       396\n",
      "   macro avg     0.6043    0.5256    0.4164       396\n",
      "weighted avg     0.6038    0.5278    0.4177       396\n",
      "\n",
      "Test Loss: 0.699 | Test Acc: 54.833 | Test macro-avg-f1: 50.633%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5349    0.8090    0.6440       199\n",
      "           1     0.6000    0.2893    0.3904       197\n",
      "\n",
      "    accuracy                         0.5505       396\n",
      "   macro avg     0.5674    0.5492    0.5172       396\n",
      "weighted avg     0.5673    0.5505    0.5178       396\n",
      "\n",
      "Test Loss: 0.707 | Test Acc: 53.250 | Test macro-avg-f1: 46.415%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5225    0.8744    0.6541       199\n",
      "           1     0.6032    0.1929    0.2923       197\n",
      "\n",
      "    accuracy                         0.5354       396\n",
      "   macro avg     0.5628    0.5336    0.4732       396\n",
      "weighted avg     0.5626    0.5354    0.4741       396\n",
      "\n",
      "Test Loss: 0.716 | Test Acc: 55.250 | Test macro-avg-f1: 48.614%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5341    0.9045    0.6716       199\n",
      "           1     0.6780    0.2030    0.3125       197\n",
      "\n",
      "    accuracy                         0.5556       396\n",
      "   macro avg     0.6060    0.5538    0.4921       396\n",
      "weighted avg     0.6057    0.5556    0.4930       396\n",
      "\n",
      "Test Loss: 0.731 | Test Acc: 53.833 | Test macro-avg-f1: 49.879%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5290    0.7789    0.6301       199\n",
      "           1     0.5728    0.2995    0.3933       197\n",
      "\n",
      "    accuracy                         0.5404       396\n",
      "   macro avg     0.5509    0.5392    0.5117       396\n",
      "weighted avg     0.5508    0.5404    0.5123       396\n",
      "\n",
      "Test Loss: 0.739 | Test Acc: 56.250 | Test macro-avg-f1: 51.326%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5429    0.8593    0.6654       199\n",
      "           1     0.6543    0.2690    0.3813       197\n",
      "\n",
      "    accuracy                         0.5657       396\n",
      "   macro avg     0.5986    0.5642    0.5233       396\n",
      "weighted avg     0.5983    0.5657    0.5240       396\n",
      "\n",
      "Test Loss: 0.723 | Test Acc: 55.917 | Test macro-avg-f1: 53.396%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5604    0.5829    0.5714       199\n",
      "           1     0.5608    0.5381    0.5492       197\n",
      "\n",
      "    accuracy                         0.5606       396\n",
      "   macro avg     0.5606    0.5605    0.5603       396\n",
      "weighted avg     0.5606    0.5606    0.5604       396\n",
      "\n",
      "Test Loss: 0.765 | Test Acc: 56.167 | Test macro-avg-f1: 52.996%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5730    0.5126    0.5411       199\n",
      "           1     0.5550    0.6142    0.5831       197\n",
      "\n",
      "    accuracy                         0.5631       396\n",
      "   macro avg     0.5640    0.5634    0.5621       396\n",
      "weighted avg     0.5641    0.5631    0.5620       396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 0.001\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for epoch in range(0,15):\n",
    "    model.load_state_dict(torch.load('data/bert_twt/model_des_wo_attn_'+str(epoch+1)+'.pt'))\n",
    "\n",
    "    test_loss, test_acc, test_f1, pred, label = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.3f} | Test macro-avg-f1: {test_f1*100:.3f}%')\n",
    "    flat_predictions = np.concatenate(pred, axis=0)\n",
    "    flat_labels = np.concatenate(label, axis=0)\n",
    "    \n",
    "    print(classification_report(flat_labels, flat_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTest Loss: 0.660 | Test Acc: 68.250 | Test macro-avg-f1: 61.566%\\nTest Loss: 0.637 | Test Acc: 72.500 | Test macro-avg-f1: 67.102%\\nTest Loss: 0.656 | Test Acc: 69.250 | Test macro-avg-f1: 62.161%\\nTest Loss: 0.581 | Test Acc: 73.500 | Test macro-avg-f1: 68.485%\\nTest Loss: 0.616 | Test Acc: 69.750 | Test macro-avg-f1: 62.418%\\nTest Loss: 0.580 | Test Acc: 73.750 | Test macro-avg-f1: 69.892%\\nTest Loss: 0.639 | Test Acc: 70.500 | Test macro-avg-f1: 63.049%\\nTest Loss: 0.574 | Test Acc: 74.000 | Test macro-avg-f1: 69.004%\\nTest Loss: 0.587 | Test Acc: 71.750 | Test macro-avg-f1: 64.628%\\nTest Loss: 0.601 | Test Acc: 73.500 | Test macro-avg-f1: 66.236%\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Test Loss: 0.660 | Test Acc: 68.250 | Test macro-avg-f1: 61.566%\n",
    "Test Loss: 0.637 | Test Acc: 72.500 | Test macro-avg-f1: 67.102%\n",
    "Test Loss: 0.656 | Test Acc: 69.250 | Test macro-avg-f1: 62.161%\n",
    "Test Loss: 0.581 | Test Acc: 73.500 | Test macro-avg-f1: 68.485%\n",
    "Test Loss: 0.616 | Test Acc: 69.750 | Test macro-avg-f1: 62.418%\n",
    "Test Loss: 0.580 | Test Acc: 73.750 | Test macro-avg-f1: 69.892%\n",
    "Test Loss: 0.639 | Test Acc: 70.500 | Test macro-avg-f1: 63.049%\n",
    "Test Loss: 0.574 | Test Acc: 74.000 | Test macro-avg-f1: 69.004%\n",
    "Test Loss: 0.587 | Test Acc: 71.750 | Test macro-avg-f1: 64.628%\n",
    "Test Loss: 0.601 | Test Acc: 73.500 | Test macro-avg-f1: 66.236%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_py36",
   "language": "python",
   "name": "grad_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
