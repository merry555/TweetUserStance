{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_emb = gsm.KeyedVectors.load_word2vec_format('user.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_em_df = pd.read_csv('./datasets/network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_em_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_em_df_men = user_em_df[['mention','mention_num']]\n",
    "user_em_df_uname = user_em_df[['username','username_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_em_df_men = user_em_df_men.rename(columns={'mention':'username','mention_num':'name_num'})\n",
    "user_em_df_uname = user_em_df_uname.rename(columns={'username':'username','username_num':'name_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_namenum = pd.concat([user_em_df_men,user_em_df_uname])\n",
    "df_namenum = df_namenum.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>name_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PalmerReport</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmckenz44559503</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeaBlackMiami</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Catteeya</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JewellE1974</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>LOALLL</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>jrbkjrbk</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>stella105eric</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>SamuelBriceHall</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>travisj75</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            username  name_num\n",
       "0       PalmerReport       418\n",
       "1    bmckenz44559503       310\n",
       "2      LeaBlackMiami       481\n",
       "3           Catteeya       219\n",
       "4        JewellE1974       641\n",
       "..               ...       ...\n",
       "723           LOALLL       676\n",
       "724         jrbkjrbk       324\n",
       "725    stella105eric       157\n",
       "726  SamuelBriceHall        74\n",
       "727        travisj75       134\n",
       "\n",
       "[728 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_namenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./datasets/test.csv\") \n",
    "train = pd.read_csv(\"./datasets/train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['username','label']]\n",
    "train = train[['username','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['label']!=2]\n",
    "train = train[train['label']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index(drop=True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates().reset_index(drop=True)\n",
    "test = test.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, stratify=train.label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test,train,valid])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates().reset_index(drop=True)\n",
    "valid = valid.drop_duplicates().reset_index(drop=True)\n",
    "test = test.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd.merge(df,df_namenum, how='outer',on='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_num.fillna(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['name_num'] = df_num['name_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_21 = df_num[df_num['name_num']==2021]\n",
    "df_num_na21 = df_num[df_num['name_num']!=2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_num_21 = df_num_21.reset_index(drop=True)\n",
    "\n",
    "n = 728\n",
    "\n",
    "for i in range(len(df_num_21)):\n",
    "    df_num_21['name_num'][i] = n\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd.concat([df_num_21, df_num_na21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train,df_num,on='username')\n",
    "\n",
    "test_data = pd.merge(test,df_num,on='username')\n",
    "\n",
    "valid_data = pd.merge(valid,df_num,on='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_net.csv',index=False)\n",
    "test_data.to_csv('test_net.csv',index=False)\n",
    "valid_data.to_csv('valid_net.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(train_data.label_x.values)\n",
    "\n",
    "labels_valid = torch.tensor(valid_data.label_x.values)\n",
    "\n",
    "labels_test = torch.tensor(test_data.label_x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_input_network(train_data):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "        for j in range (0,train_data.shape[0]):\n",
    "            label =  [int(train_data.label_x[j])]\n",
    "            if(str(train_data['name_num'][i]) not in net_emb ):\n",
    "                net_emb[str(train_data['name_num'][i])] = np.zeros(300)\n",
    "            net_emb_tensor = torch.Tensor(net_emb[str(train_data['name_num'][i])])\n",
    "            training_data.append(net_emb_tensor.unsqueeze(0))\n",
    "            break\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/grad_py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#####prepare training data for neural net #########\n",
    "training_data_net =  nn_input_network(train_data)\n",
    "#####prepare validation data for neural net #########\n",
    "validation_data_net =  nn_input_network(valid_data)\n",
    "#####prepare Testing data for neural net #########\n",
    "testing_data_net =  nn_input_network(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists into tensors.\n",
    "training_data_net = torch.cat(training_data_net, dim=0)\n",
    "validation_data_net = torch.cat(validation_data_net, dim=0)\n",
    "testing_data_net = torch.cat(testing_data_net, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(training_data_net, labels_train)\n",
    "val_dataset = TensorDataset(validation_data_net, labels_valid)\n",
    "test_dataset = TensorDataset(testing_data_net, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache() \n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    #device = torch.device(\"cuda\") # select the zeroth GPU with this line: gpu = 0\n",
    "    #device = torch.cuda.set_device(1)  #wrong provide device = None  \n",
    "    device = torch.device(1) #(use cuda device 1) for gpu = 1\n",
    "    torch.cuda.set_device(device)\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 17 15:18:49 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\r\n",
      "| 30%   49C    P8    21W / 250W |     11MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\r\n",
      "| 36%   58C    P8    14W / 250W |     11MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 36%   58C    P8    20W / 250W |     11MiB / 11016MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.cuda.current_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([32, 300])\n",
    "        z1 = self.fc1(X)\n",
    "        #print('z1', z1, z1.size()) # torch.Size([32, 150])\n",
    "        #h1 = F.relu(z1) \n",
    "        return z1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_net = NetworkMLP()\n",
    "        self.fc2 = nn.Linear(150, 3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,  x_n): \n",
    "        prediction_net = self.model_net(x_n)\n",
    "        prediction_net = self.dropout(prediction_net) # add dropout\n",
    "        output = self.fc2(F.relu(prediction_net))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "\n",
    "model = UserModel(DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(),\n",
    "#                   lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "\n",
    "    return acc, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net = batch[0].to(device)\n",
    "        label = batch[1].to(device)\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        #predictions = model(text)\n",
    "        predictions = model( net)\n",
    "        #pred = model(text)\n",
    "        #print(\"pred\", pred.size()) #torch.Size([32, 3])\n",
    "        #predictions = torch.argmax(pred, dim = 1)\n",
    "        #print(\"predictions\", predictions, predictions.size()) #torch.Size([32, 3])\n",
    "        #loss = criterion(predictions, batch.label)\n",
    "        loss = criterion(predictions, label)\n",
    "        #print(\"loss\", loss)\n",
    "        #acc = binary_accuracy(predictions, batch.label)\n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "        #print(\"acc\", acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            \n",
    "            net = batch[0].to(device)\n",
    "            label = batch[1].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model( net)\n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved in epoch : 1\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.602 | Train Acc: 59.18 | Train macro-avg-f1: 58.45%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 61.88 |  Val. macro-avg-f1: 58.50%\n",
      "best model saved in epoch : 2\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.600 | Train Acc: 62.24 | Train macro-avg-f1: 61.52%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 61.82 |  Val. macro-avg-f1: 58.45%\n",
      "best model saved in epoch : 3\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.603 | Train Acc: 60.37 | Train macro-avg-f1: 59.48%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 61.76 |  Val. macro-avg-f1: 58.40%\n",
      "best model saved in epoch : 4\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.608 | Train Acc: 61.96 | Train macro-avg-f1: 60.99%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 61.76 |  Val. macro-avg-f1: 58.40%\n",
      "best model saved in epoch : 5\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.599 | Train Acc: 60.55 | Train macro-avg-f1: 59.72%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 61.63 |  Val. macro-avg-f1: 58.29%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "per_epoch_train_loss = []\n",
    "per_epoch_val_loss = []\n",
    "per_epoch_train_f1 = []\n",
    "per_epoch_val_f1 = []\n",
    "per_epoch_train_acc = []\n",
    "per_epoch_val_acc = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    per_epoch_train_loss.append(train_loss)\n",
    "    per_epoch_val_loss.append(valid_loss)\n",
    "    per_epoch_train_f1.append(train_f1)\n",
    "    per_epoch_val_f1.append(valid_f1)\n",
    "    per_epoch_train_acc.append(train_acc)\n",
    "    per_epoch_val_acc.append(valid_acc)\n",
    "    \n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"best model saved in epoch :\", epoch+1 )\n",
    "    torch.save(model.state_dict(), 'data/unet/net_utype'+str(epoch+1)+'.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        preds , true_labels = [], []\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            text = batch[0].to(device)\n",
    "            label = batch[1].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(text)\n",
    "            \n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds.append(torch.argmax(predictions, dim = 1).to('cpu').numpy())\n",
    "            true_labels.append(label.to('cpu').numpy())\n",
    "            \n",
    "                        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator), preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.621 | Test Acc: 61.699 | Test macro-avg-f1: 56.848%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7722    0.3065    0.4388       199\n",
      "           1     0.5647    0.9086    0.6965       197\n",
      "\n",
      "    accuracy                         0.6061       396\n",
      "   macro avg     0.6684    0.6076    0.5677       396\n",
      "weighted avg     0.6689    0.6061    0.5670       396\n",
      "\n",
      "Test Loss: 0.620 | Test Acc: 61.699 | Test macro-avg-f1: 56.848%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7722    0.3065    0.4388       199\n",
      "           1     0.5647    0.9086    0.6965       197\n",
      "\n",
      "    accuracy                         0.6061       396\n",
      "   macro avg     0.6684    0.6076    0.5677       396\n",
      "weighted avg     0.6689    0.6061    0.5670       396\n",
      "\n",
      "Test Loss: 0.620 | Test Acc: 61.699 | Test macro-avg-f1: 56.848%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7722    0.3065    0.4388       199\n",
      "           1     0.5647    0.9086    0.6965       197\n",
      "\n",
      "    accuracy                         0.6061       396\n",
      "   macro avg     0.6684    0.6076    0.5677       396\n",
      "weighted avg     0.6689    0.6061    0.5670       396\n",
      "\n",
      "Test Loss: 0.620 | Test Acc: 61.699 | Test macro-avg-f1: 56.848%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7722    0.3065    0.4388       199\n",
      "           1     0.5647    0.9086    0.6965       197\n",
      "\n",
      "    accuracy                         0.6061       396\n",
      "   macro avg     0.6684    0.6076    0.5677       396\n",
      "weighted avg     0.6689    0.6061    0.5670       396\n",
      "\n",
      "Test Loss: 0.620 | Test Acc: 61.699 | Test macro-avg-f1: 56.848%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7722    0.3065    0.4388       199\n",
      "           1     0.5647    0.9086    0.6965       197\n",
      "\n",
      "    accuracy                         0.6061       396\n",
      "   macro avg     0.6684    0.6076    0.5677       396\n",
      "weighted avg     0.6689    0.6061    0.5670       396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for epoch in range(0,5):\n",
    "    model.load_state_dict(torch.load('data/unet/net_utype'+str(epoch+1)+'.pt'))\n",
    "    test_loss, test_acc, test_f1, pred, label = evaluate_test(model, test_dataloader, criterion)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.3f} | Test macro-avg-f1: {test_f1*100:.3f}%')\n",
    "    flat_predictions = np.concatenate(pred, axis=0)\n",
    "    flat_labels = np.concatenate(label, axis=0)\n",
    "    \n",
    "    print(classification_report(flat_labels, flat_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
